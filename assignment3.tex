\documentclass[12pt, a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage{geometry}
\usepackage[colorlinks=false,
            linkcolor=blue,
            anchorcolor=blue,
            citecolor=blue]{hyperref}
\geometry{left=2cm, right=2cm, top=2cm, bottom=2cm}

\title{COMP2550 Assignment3}
\author{Xingjian Leng(u7136359), Xuning Tan(u6792826), Zhe Chen(u6484911)}
\date{May 2021}

\begin{document}
    
\maketitle
\subsection{Q1}
The previous Visual-Linguistic problems (VLP) methods combine image and text features as model input for multimodal pre-training. They mostly designed to use a transformer self-attention mechanism to learn image text semantic alignment by brute force but cannot offer any modal clues. \cite{DBLP:journals/corr/abs-2004-06165} After Oscar was proposed, this method uses object tags as anchor points to align images and language modalities in a shared semantic space, handling Vision + language multimodal well problem. [3] However, we found that Oscar still uses the most primitive transformer layer to achieve their results, in this case, we would like to use Neural Architecture Search (NAS) to search for better Oscar transformers automatically. Considering Oscar uses the most concise Transformer, search for a new Transformer through NAS may be quite complicated. Still, we hope to create a new, slightly more complex architecture to improve the accuracy and efficiency of the existing Oscar.

\subsection*{Q2}

\par\noindent For current Visual-Linguistic problems, the most widely used method is deep learning. Neural networks are the core of deep learning and have been applied to many Computer Vision and Natural Language Processing models. They have shown their capability to model complex patterns and prediction problems.

\par\noindent \newline In machine learning models, hyperparameters could notably affect the performance. Therefore, the choice of hyperparameters is essential in improving performance. However, there are a large number of hyperparameters in neural networks. The neural network architecture is one of the hyperparameters (i.e., how layers could be connected to build the whole network). We are motivated by applying MMNAS \cite{DBLP:journals/corr/abs-2004-12070} to MCAN \cite{DBLP:journals/corr/abs-1906-10770} to search for the optimal combination of attention modules. Thus, we decided to use Neural Architecture Search (NAS) in searching for the optimal transformer architecture in OSCAR \cite{DBLP:journals/corr/abs-2004-06165} to further improve its performance.

\par\noindent \newline In terms of the transformer layers in the current OSCAR \cite{DBLP:journals/corr/abs-2004-06165} model. They applied traditional universal transformers \cite{DBLP:journals/corr/VaswaniSPUJGKP17} after the token extraction step. This model is still the state-of-the-art model in the VQA field even though no advanced transformer is applied to it. It motivated our group because by using neural architecture search, we may find a superior transformer architecture especially for OSCAR. A new OSCAR model with higher accuracy or with fewer parameters may emerge.


\subsection*{Q3 Overall summary}
\par\noindent Our group is going to apply Neural Architecture Search(NAS) vision-language tasks. We will mainly focus on searching for better substitute transformer parameters and architectures of the OSCAR \cite{DBLP:journals/corr/abs-2004-06165} model, hence we will divide our research topic's summary of state of the art into 
two parts, one is for original transformer in OSCAR \cite{DBLP:journals/corr/abs-2004-06165} model, another is for the most advanced NAS 
architecture, MMNAS.

\subsubsection*{OSCAR}

\par\noindent As the title of the original paper infers, OSCAR \cite{DBLP:journals/corr/abs-2004-06165} stands for Object-Semantics Aligned Pre-training for Vision-Language Tasks, it is state of the art on vision-language tasks. Vision-language pre-training models before OSCAR \cite{DBLP:journals/corr/abs-2004-06165} mostly connect image and text features together as input and use them with self-attention mechanism to learn the semantic information in brutal force. Such process is normally done with multi-layer Transformers. Since extracting semantic alignments is basically like a headless chicken, essential information between visual and text regions might be ignored.

\par\noindent \newline Previous methods majorly apply multi-layer self-attention Transformers to learn cross-modal background representations, with singular embedding of each modality, which makes result of VLP tasks mostly affected by the input singular embedding. Nevertheless, VLP is naturally a weakly-supervised learning task since the explicit information between text and images are always not well labeled. On the other hand, visual features extracted with Fast R-CNN object detectors  are usually over-sampled, which result in ambiguities for the extracted visual embedding. To improve this situation, OSCAR \cite{DBLP:journals/corr/abs-2004-06165} innovatively introduces \verb|anchor point| to help the model learn semantic alignments between images and texts. \verb|anchor point| is like an object tags detected in images. They treat the training samples as a combination of word, images and \verb|anchor point|.

\subsubsection*{MMnas}
\par\noindent The deep learning community is taking a transition from human-designed neural architecture to automatically self-designed neural architecture, known as AutoML. Neural Architecture Search is a subset of AutoML, it majorly focuses on automated Neural Network architecture selection and creation, past year have seen has a great number of successful applications of NAS. A NAS procedure can be divided into three components, (1) Search Space, (2)Candidate Evaluation Method and (3)Optimization Method. Search defines the potential network that can be examined to produce the final desired Neural Network. The candidate evaluation method is for comparing the intermediate result and help choose various options among the search space. The optimization method defines how to actually explor the search space, which is essential to the search efficiency and effectiveness of the result architecture.

\par\noindent \newline MMnas \cite{DBLP:journals/corr/abs-2004-12070} is a generalized deep multi-modal neural architecture search framwork for multi-model learning task, the underlying thought is based on BERT model from the natural language processing (NLP), however it is more efficient as it does not require as much data compared to BERT, the huge computing power required by BERT hinders its application in practical situations. Inspired by MCAN model, it firstly searches a set of primitive operations, including feed-forwardnetwork (FFN), self-attention (SA), relation self-attention (RSA) and guided-attention (GA) as the basic unit, they used a unified encoder-decoder backbone through directly sending features into the encoder and decoder. They also designed task-specific heads to different visual linguistic tasks, such as visual question answering and image-text matching(ITM).

\par\noindent \newline As a result, With the standard visual features, MMnas \cite{DBLP:journals/corr/abs-2004-12070} achieves an outstanding improvements on existing hand-crafted models accross different datasets. When applied powerful visual features, MMnas achieved state of the art performance accross all datasets. Thanks to task-specific heads and unified encoder-decoder, MMnas \cite{DBLP:journals/corr/abs-2004-12070} has the ability to automatically learn the optimal architectures of different tasks. 

\subsubsection*{BossNAS}
BossNAS is a block-wisely self-supervised Neural Architecture Search method with hybrid CNN-transformers. It solves in-accurate architecture rating problem due to large weight-sharing space and biased supervision, which is quite common in previous methods.\\ Neural Architecture Search (NAS) is aimed at reducing the human effort in network architecture design, to achieve this, it basically searches for optimal architectures within a predefined search space. As stated in [BossNas], those weight-sharing NAS methods encode the whole search space as a weight-sharing supernet to avoid repetitive training of candidate networks, thus largely reducing the search cost. However, architecture search spaces with layer-level granularity grow exponentially with increased network depth, which results in reduce of calculation efficiency. To solve the problems caused by large weight-sharing space and biased supervision, they proposed a \verb|ensemble bootstrapping| as a self-supervised representation learning scheme to find the optimal for the each block of their supernet. Besides, an unsupervised \verb| evaluation metric| is proposed for the searching stage, to ensure fairness. They also designed a fabric-likehybrid CNN-transformer search space(\verb|HyTra|) as a case study for the network to evaluate the method. As a result, BossNet-T achieves an accuracy of $82.2\%$ on ImageNet, and is also $2.1\%$ higher than EfficientNet with comparable compute time.

\subsection*{Q3 OSCAR summary by Xingjian Leng}
\par\noindent \newline Part of our research topic is related to VQA, the current state-of-the-art VQA model is "Object-Semantics Aligned Pre-training for Vision-Language Tasks" (OSCAR) \cite{DBLP:journals/corr/abs-2004-06165}.

\par\noindent \newline In OSCAR \cite{DBLP:journals/corr/abs-2004-06165}, they firstly extract visual and semantic features from input images and sentences. Then, they introduced a new idea using "Object-tag", motivated by the fact that salient image features would also appear in sentences as pairs. Object tags are derived by aligning text tokens on the anchor points generated from the Faster R-CNN. They contain information about the feature alignment between two modalities. 

\par\noindent \newline Next, visual features, object tags and text tokens are combined as triples and send to the multi-layer transformers. Transformers are widely used in models with multi-modal features. For transformer \cite{DBLP:journals/corr/VaswaniSPUJGKP17} in each layer, it is consist of a multi-head attention layer and a feed-forward network. In the multi-head attention layer, the output is calculated by multiplying the input Value matrix with the attention probability. The attention probability shows how the Query matrix is related to the Key matrix. The purpose of the multi-head attention layer is to calculate the weighted sum of encoded outputs, namely the context. Then, the output from the attention layer is sent to the feed-forward neural network to generate the output. 

\par\noindent \newline One advantage to use transformers is that they can be pre-trained with certain visual-linguistic tasks. For pre-training, one common method is to mask some tokens and train the neural network to figure out the mask tokens. After pre-training, the model can be trained with a specific downstream task. Thus, models using transformers could usually tackle more than one type of visual-linguistic tasks. It is suggested in \cite{DBLP:journals/corr/abs-2101-11562} that pre-training could boost the performance of models. 

\par\noindent \newline The pre-training process of OSCAR \cite{DBLP:journals/corr/abs-2004-06165} is in self-supervised manner. Input triples can be viewed in two perspectives: 1) Dictionary View, 2) Modality View. In terms of the Dictionary View, word tokens and object tags are combined and considered as language part. The image tokens are treated as the image part. Each language token has 15\% of probability to be masked. Other tokens are used to predict the masked tokens. The Masked Token Loss is used as the loss function for Dictionary View pre-training. For the Modality View, object tags and image tokens are considered as image part. Object tags have 50\% probability to be polluted with a different tag randomly selected from the dataset. Other tokens are used to recover the polluted tags. The loss function for Modality View pre-training is Contrastive Loss. The objective during the OSCAR model pre-training is to minimize the sum of the Masked Token Loss and Contrastive Loss.

\subsection*{Q4}
\par\noindent \newline During the teaching break, our group focused on applying light-weighted backbones to current state-of-the-art VQA models. For example, replace the Faster R-CNN \cite{DBLP:journals/corr/RenHG015} for image feature extraction with MobileNet \cite{DBLP:journals/corr/HowardZCKWWAA17}. MobileNet \cite{DBLP:journals/corr/HowardZCKWWAA17} is one type of light-weighted network. It requires fewer parameters for training. Thus, it could be easily applied to embedded systems and deployed. In the first tutorial after the teaching break, we discussed our thoughts with our tutor. However, the tutor mentioned that applying light-weighted networks to current state-of-the-art models will definitely speed up the training process. It cannot be considered as innovation. Therefore, our group decided to change the research topic.

\par\noindent \newline Our group then read certain papers about how to improve the performance of neural networks. We were inspired by the MMnas paper. We finalized our research topic to apply Neural Architecture Search to the current state-of-the-art visual-question answering model. 



\par\noindent \newline After deciding our research topic, we read more paper in the following weeks because we lacked the basic knowledge of how neural architecture search works. There are many search algorithms for neural architecture search. For example, in "Neural architecture search with reinforcement learning" \cite{DBLP:journals/corr/ZophL16}, researchers implement neural architecture with reinforcement learning algorithms. In "Progressive Neural Architecture Search" \cite{DBLP:journals/corr/abs-1712-00559}, the Sequential Model-Based Global Optimization (SMBO) algorithm for NAS. Also, in "Deep Multimodal Neural Architecture Search" \cite{DBLP:journals/corr/abs-2004-12070}, a Neural SuperNet is used for searching for the best combination of operations, where operations are sub-neural network architectures. 
\par\noindent \newline There are pros and cons for each searching algorithm of NAS. In terms of using reinforcement learning in NAS, a reward (could be either positive or negative) is given for choosing any of the operation (sub-neural network). This reward affects the AutoML to decide whether this operation would be selected next time. 
\subsection{Q3 MMnas summary by Zhe Chen}
Nowadays, most of the work focus on how to manually design the network structure on a single task, but this will greatly increase the cost of manpower and time, such design usually involves human expertise, as well as unavoidably human biases. It is even more difficult to generalize between multiple tasks, the key to deep multi-modal model learning is to design an effective neural network structure. One of the existing multimodal frameworks, known as BERT\cite{DBLP:journals/corr/chen2019uniter},  uses a large-scale pre-train multimodal, so that this method is able to achieve good results on the various multimodal learning tasks, but their computational cost is also very high, which makes their applicability is not strong enough. This paper proposes a multi-modal multi-task learning framework, deep multimodal neural architecture search framework, MMnas. The obtained MMnasNet is the champion in three multi-modal learning tasks, where more than 5 data sets are tested. It is significantly better than the existing models including processing VQA, ITM, and VG. The implementation of the MMnas starts with determining the operators, including self-attention (SA), guided-attention (GA), feed-forward network (FFN) and relation self-attention (RSA). The next step is to construct a deep encoder-decoder unified backbone for processing pictures and text information. On this basis, task-specific heads to process different multimodal tasks are also added. Finally, learn by using the gradient-based NAS algorithm , Select the most effective structure for different tasks. This method can more effectively capture the characteristics of each task, and the parameterization efficiency is higher. Through extensive experiments, excellent results have been achieved, highlighting the effectiveness and generalizability of this method framework. It is also hoped that this approach can stimulate future research on multimodal learning.
\subsection{Q3 BossNAS summary by Xuning Tan}
BossNas has inspired our group to find the possible operators and decide the search space of our targeted transformer. It is a block-wisely self-supervised Neural Architecture Search method that achieves superior architecture rating accuracy.\\
\paragraph{Block-wise NAS}Although common sample-based NAS methods promises accurate architecture rating, they are usually computationally expensive. Those so called weight sharing rating scheme, adapted from One-shot NAS[], encodes the whole search space and brings a significant reduction of computation cost, it block-wisely factorize the search space in dimension of the depth to find a way out of the dilemma of NAS, which would reduce the weight-sharing space and maintain the original size of the search space at the same time. To factorize the search space, previous works [36,46] use a pre-trained teacher model to provide block-wise supervision, it inevitably leads to architectural bias, which leads to a less satisfying architecture rating accuracy. In order to improve the above problems, they proposed a new scheme without using a teacher model.\\
They firstly uses supernet to replace the original teacher network from block-wise NAS, and in result they construct \verb|Siamese supernets|. To ensure training fairness and consistency of different sub-networks, they proposed an unsupervised supernet training scheme called \verb|ensemble bootstrapping| to optimise paths for weight sharing in \verb|Siamese supernets| by forming a common objective for all paths. In addition, labelled data is not needed for the whole training process.
Previous self-supervised training methods use supervised methods to evaluate the trained models, such as linear evaluation, new-shot classification, etc. BossNas aims to build a unsupervised NAS method without using such label-dependent supervised evaluation metrics. They propose a fair and valid unsupervised evaluation metric for architecture search. On the one hand, each sample produces a fixed pair of augmented views of the data for evaluation, on the other hand, evolutionary algorithm was also taken in to consideration, all the population are used as target architectures to produce a probability ensemble without randomness as a target for evaluation.\\
\paragraph{HyTra} They propose a fabric-like hybrid CNN-Transformer  search space, HyTra. First, the authors use the residual bottleneck(ResConv) from ResNet[] as the convolutional candidate building block. Then they use a module similar to the implicit position encoding in CPVT to replace the relative position encoding branch in the BoTNet building block in order to reduce the computational complexity of the transformer candidate building block. The implicit position encoding module is also added to ResConv and is responsible for downsampling, allowing the weighting of inputs at different scales in the fabric-like space to be shared.


\newpage
\bibliographystyle{IEEEtran}
\bibliography{ref}

\end{document}