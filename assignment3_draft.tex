\documentclass[12pt, a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage{geometry}
\usepackage[colorlinks,
            linkcolor=blue,
            anchorcolor=blue,
            citecolor=blue]{hyperref}
\geometry{left=2cm, right=2cm, top=2cm, bottom=2cm}

\title{COMP2550 Assignment3}
\author{Xingjian Leng(u7136359), Xuning Tan(u6792826), Zhe Chen(u)}
\date{May 2021}

\begin{document}
    
\maketitle

\subsection*{Q2}

\par\noindent For current Visual-Linguistic problems, the most widely used method is deep learning. Neural networks are the core of deep learning and have been applied to many Computer Vision and Natural Language Processing models. They have shown their capability to model complex patterns and prediction problems.

\par\noindent \newline In machine learning models, hyperparameters could notably affect the performance. Therefore, the choice of hyperparameters is essential in improving performance. However, there are a large number of hyperparameters in neural networks. The neural network architecture is one of the hyperparameters (i.e., how layers could be connected to build the whole network). We are motivated by applying MMNAS \cite{DBLP:journals/corr/abs-2004-12070} to MCAN \cite{DBLP:journals/corr/abs-1906-10770} to search for the optimal combination of attention modules. Thus, we decided to use Neural Architecture Search (NAS) in searching for the optimal transformer architecture in OSCAR \cite{DBLP:journals/corr/abs-2004-06165} to further improve its performance.

\par\noindent \newline In terms of the transformer layers in the current OSCAR \cite{DBLP:journals/corr/abs-2004-06165} model. They applied traditional universal transformers \cite{DBLP:journals/corr/VaswaniSPUJGKP17} after the token extraction step. This model is still the state-of-the-art model in the VQA field even though no advanced transformer is applied to it. It motivated our group because by using neural architecture search, we may find a superior transformer architecture especially for OSCAR. A new OSCAR model with higher accuracy or with fewer parameters may emerge.
\subsection{Q3}
Our group is going to apply Neural Architecture Search(NAS) vision-language tasks. 
We will mainly focus on searching for better substitute transformer parameters and architectures 
of the OSCAR model, hence we will divide our research topic's summary of state of the art into 
two parts, one is for original transformer in OSCAR model, another is for the most advanced NAS 
architecture, MMNAS. \\
\subsubsection{OSCAR}
As the title of the original paper infers, OSCAR stands for Object-Semantics Aligned Pre-training for Vision-Language Tasks, it is state of the art on vision-language tasks. Vision-language  pre-training models before OSCAR mostly connect image and text features together as input and use them with self-attention mechanism to learn the semantic information in brutal force. Such process is normally done with multi-layer Transformers. Since extracting semantic alignments is basically like a headless chicken, essential information between visual and text regions might be ignored. \\
Previous methods majorly apply multi-layer self-attention Transformers to learn cross-modal background representations, with singular embedding of each modality, which makes result of VLP tasks mostly affected by the input singular embedding. Nevertheless, VLP is naturally a weakly-supervised learning task since the explicit information between text and images are always not well labeled. On the other hand, visual features extracted with Fast R-CNN object detectors  are usually over-sampled, which result in ambiguities for the extracted visual embedding. To improve this situation, OSCAR innovatively introduces \verb|anchor point| to help the model learn semantic alignments between images and texts. \verb|anchor point| is like an object tags detected in images. They treat the training samples as a combination of word, images and \verb|anchor point|. \\
\bibliographystyle{IEEEtran}
\bibliography{ref}

\end{document}